{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、项目背景¶\n",
    "    由于每秒钟都有大量的推文在传播，所以很难判断一个特定推文背后的情感是否会影响一个人或一个公司的品牌，可能因为它具有积极性或者因为它带有负面的基调。用语言捕捉情绪是很重要的，尤其是当几秒钟内就需要做出决策或判断的时候。但是，到底是哪些词语主导了情感的表达呢? 在这次比赛中，数据集名为“情绪分析:带有情感标签的推文中的情感”。目标是建立一个模型来做同样的事情——查看给定tweet的情感标记，并挑选出推文中最能支撑这个情感的单词或短语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tokenizers\n",
    "from tqdm.autonotebook import tqdm\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd394019bf0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、数据前处理\n",
    "## 1.数据清洗\n",
    "     查看数据集，包含文本ID（textID）、文本内容（text）、代表情感的文本（selected_text）以及该文本所对应的情感（sentiment）。总共包含27481条训练集以及3534条需要预测的测试集。将所有的文本小写化，并且删去所有空值（个数极少）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 4) (3534, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                            text  \\\n",
       "0  cb774db0d1             I`d have responded, if I were going   \n",
       "1  549e992a42   Sooo SAD I will miss you here in San Diego!!!   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = pd.read_csv('../input/mydata/tw_train2.csv')\n",
    "ds_test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "print(ds_train.shape,ds_test.shape)\n",
    "ds_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)  #删去. * ? \\\n",
    "\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)  #。网址后面的字母会删去，\\S匹配任意非空白符， ？是前面操作无或一次 \n",
    "    #text = re.sub('https?://\\S+|www\\S+', '', text)    \n",
    "\n",
    "    \n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    #text = re.sub('\\w*\\d\\w*', '', text)  #数字和后面的字母\n",
    "    text = re.sub('\\d', '', text)    #只删除数字\n",
    "    return text\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Cleaning and parsing the text.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    nopunc = clean_text(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
    "    combined_text = ' '.join(tokenized_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>i`d have responded, if i were going</td>\n",
       "      <td>i`d have responded, if i were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>sooo sad i will miss you here in san diego!!!</td>\n",
       "      <td>sooo sad</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                i`d have responded, if i were going   \n",
       "1  549e992a42      sooo sad i will miss you here in san diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  i`d have responded, if i were going   neutral  \n",
       "1                             sooo sad  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        sons of ****,  negative  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#标点，网址\n",
    "ds_train['text'] = ds_train['text'].apply(str).apply(lambda x :x.lower())\n",
    "ds_test['text'] = ds_test['text'].apply(str).apply(lambda x :x.lower())\n",
    "ds_train['selected_text'] = ds_train['selected_text'].apply(str).apply(lambda x :x.lower())\n",
    "ds_train.loc[ds_train['selected_text']=='#name?', 'selected_text'] = None\n",
    "#有很多非nan的空值\n",
    "#pd.set_option('display.max_colwidth', 200)\n",
    "ds_train.replace('', np.nan, inplace=True)\n",
    "#ds_test.replace('', np.nan,inplace=True)\n",
    "ds_train.dropna(axis=0, how ='any',inplace=True) \n",
    "ds_train.head()\n",
    "#ds_test.dropna(axis=0, how='any',inplace=True)\n",
    "#网址数据有很大干扰性，且常报错，全部删去\n",
    "#ds_web = ds_train.loc[ds_train.text.str.contains('www\\.|http?//|https//')]\n",
    "#ds_dropweb = ds_train[~ds_train.text.str.contains('www\\.|http|https//')]\n",
    "#ds_train = ds_dropweb\n",
    "#ds_train.loc[ds_train.text.str.contains('http|www\\.'), 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file='../input/roberta/roberta-base-vocab.json',\n",
    "                      merges_file='../input/roberta/roberta-base-merges.txt',\n",
    "                      lowercase=True,\n",
    "                      add_prefix_space=True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Data Loader\n",
    "    将每条文本转化为token，将问题视为Question Answer(QA),将sentiment作为question、在文本tweet中寻找答案selected_text，并且标记出答案在文本中的起始和结束位置。其中需要将sentiment和tweet连接，中间用【SEP】分隔，并且使合并的句子填充至同一长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "\n",
    "  def __init__(self,ds, tokenizer, max_len, mod):\n",
    "    self.ds = ds\n",
    "    self.tweets = np.array(self.ds['text'])\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "    self.sentiments = np.array(self.ds['sentiment'])\n",
    "    self.mod = mod\n",
    "    if self.mod != 'test':\n",
    "      self.selected_texts = np.array(self.ds['selected_text'])\n",
    "  def __len__(self):\n",
    "    return len(self.tweets)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "  ###需要输出：原推特，推特input_ids，padding与否的标识mask，两个句子的区分token_type_ids, sentiment_text的起始和结束，offset，sentiment\n",
    "  #通过迭代器item，写出每一行的内容\n",
    "    #将文本变为str，以下长度计数会按字母数数，如：len（str（my））=2\n",
    "    tweet = str(self.tweets[item])\n",
    "    sentiment = str(self.sentiments[item])\n",
    "    #print(item)  #找出错数据的时候开启\n",
    "    #print(tweet)\n",
    "    #print(self.selected_texts[item])\n",
    "    #测试模式：\n",
    "    if self.mod == 'test':\n",
    "      data = self.get_test_data(tweet, self.tokenizer, self.max_len, sentiment)\n",
    "      return data\n",
    "    #训练模式：\n",
    "    selected_text = str(self.selected_texts[item])\n",
    "    data = self.process_data(tweet, selected_text, self.tokenizer, self.max_len, sentiment)\n",
    "\n",
    "\n",
    "    return data\n",
    "  def process_data(self,tweet, selected_text, tokenizer, max_len, sentiment):\n",
    "    \"\"\"\n",
    "    实现找到selected_text对应的token在原tweet token所在的位置（先找到文本的对应位置，再转化为token）:\n",
    "    1.通过原文本的一一对应关系找到selected_text的起始引索，\n",
    "    2.新开一个列表char_targets用来记录tweet各个位置是否为selected_text的一部分，是为1，否为0；\n",
    "    3.将文本tokenize，token的个数肯定小于字母的个数，但是可以得到一个包含token位于原文本起始位置的元组offsets\n",
    "    4.通过char_targets与offsets判断每个token是否是target的一部分，最终得到target的起始token位置\n",
    "    \"\"\"\n",
    "    #通过答案案的第一个字母和答案的长度判断答案是否在原文中，将开始和结束的位置提出，返回一个用字母计数的长度\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "    #将答案的位置全部赋值为1，其他位置赋值为0，用于以下判断\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    input_ids_orig = tok_tweet.ids\n",
    "    tweet_offsets = tok_tweet.offsets #找到每个token（单词）的出现的起始位置（一个字母算1，空格单词结尾，也记为1）和结束位置(start,end)\n",
    "\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:          #判断每个token（单词）是否为答案的一部分\n",
    "            target_idx.append(j)\n",
    "    \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "#########################################################################################################\n",
    "    #直接写出sentiment（question）的token,手动加到input_ids里面\n",
    "    sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "    \n",
    "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1) \n",
    "    #三个0代表【cls】【sentiment】【sep】，用于区分前后两个句子，相当于segementation\n",
    "    mask = [1] * len(token_type_ids) #用于区分是否是padding，padding为0\n",
    "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
    "    targets_start += 4\n",
    "    targets_end += 4\n",
    "    #如果句子小于设定的长度，用0padding\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'mask': torch.tensor(mask, dtype=torch.long),\n",
    "        'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        'targets_start': torch.tensor(targets_start, dtype=torch.long),\n",
    "        'targets_end': torch.tensor(targets_end, dtype=torch.long),\n",
    "        'tweet_text': tweet,\n",
    "        'selected_text': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': torch.tensor(tweet_offsets, dtype=torch.long)\n",
    "    }\n",
    "  def get_test_data(self, tweet, tokenizer, max_len, sentiment):\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7979\n",
    "    }\n",
    "    input_ids_orig = tokenizer.encode(tweet).ids   #这里的ids是否有必要\n",
    "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1) \n",
    "    #三个0代表【cls】【sentiment】【sep】，用于区分前后两个句子，相当于segementation\n",
    "    mask = [1] * len(token_type_ids) #用于区分是否是padding，padding为0\n",
    "    #tweet_offsets = [(0, 0)] * 3 + tweet_offsets + [(0, 0)]\n",
    "    #targets_start += 3\n",
    "    #targets_end += 3\n",
    "    #如果句子小于设定的长度，用0padding\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        #tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'mask': torch.tensor(mask, dtype=torch.long),\n",
    "        'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        'tweet_text': tweet,\n",
    "        'sentiment': sentiment\n",
    "    }\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size, mod=None):\n",
    "    ds = TweetDataset(ds=df, \n",
    "    tokenizer = tokenizer, \n",
    "    max_len = max_len,\n",
    "    mod = mod\n",
    "  )\n",
    "  \n",
    "    return DataLoader(ds,\n",
    "    batch_size = batch_size,\n",
    "    num_workers = 0\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(ds_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "#val_data_loader = create_data_loader(eval, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(ds_test, tokenizer, MAX_LEN, BATCH_SIZE, mod='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、Roberta-base Models\n",
    "## 1. Model_1\n",
    "    使用roberta模型的最后两层输出，合并大小为(batch_size,MAX_LEN,2*hidden_state)。将模型的输出进行dropout(rate=0.3)之后连接一个全连接层，全连接层的大小为(2*hidden_state, 2),得到每个位置的作为起始位置和结束位置的两个分数，大小皆为(batch_size,MAX_LEN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Roberta_model_1(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(Roberta_model_1, self).__init__()\n",
    "        self.bert = transformers.RobertaModel.from_pretrained('../input/roberta/roberta-base-pytorch_model.bin',config=config)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.start_end = nn.Linear(self.bert.config.hidden_size*2, 2)\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _,_, out = self.bert(\n",
    "          input_ids=ids,\n",
    "          attention_mask=mask,\n",
    "          token_type_ids=token_type_ids\n",
    "        )      #tuple:(embedding(batch_size,sequence_length,hidden_size),output of each layer(batch_size,sequence_length,hidden_size))\n",
    "        output = torch.cat((out[-1], out[-2]), dim=-1) #(batch_size, sequence_length, hidden_size*2)\n",
    "        output = self.drop(output)\n",
    "        output = self.start_end(output) #(batch_size, sequence_length, 2)\n",
    "        start, end = torch.split(output, split_size_or_sections=1, dim=-1)  #(batch_size, sequence_length, 1)\n",
    "        start = torch.squeeze(input=start, dim=-1)\n",
    "        end = torch.squeeze(input=end, dim=-1)\n",
    "\n",
    "        return start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Model_2\n",
    "    将roberta模型的倒数四层输出做平均处理，大小为(batch_size,MAX_LEN,hidden_size)，将模型的输出进行dropout(rate=0.5)之后连接一个全连接层，大小为(hidden_size, 2)，输出的两个元素分别代表答案在推特文本中的起始位置和结束位置，大小皆为(batch_size,MAX_LEN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Roberta_model_2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Roberta_model_2, self).__init__()  \n",
    "        self.roberta = transformers.RobertaModel.from_pretrained(\n",
    "            '../input/roberta/roberta-base-pytorch_model.bin', config=config)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(config.hidden_size, 2)\n",
    "        nn.init.normal_(self.fc.weight, std=0.02)\n",
    "        nn.init.normal_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, _, hs = self.roberta(input_ids, attention_mask)\n",
    "        x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n",
    "        x = torch.mean(x, 0)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        start_logits, end_logits = x.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "                \n",
    "        return start_logits, end_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Model_3:连接cnn的模型\n",
    "    使用Roberta最后一层学到的hidden_state，大小为（batch_size,sequence_length,768),Dropout(rate=0.3),然后连接kernel_size为128的1D卷积层，对768维的hidden_state进行卷积，stride为1，目的是提取出能代表文本起始与结束位置的有效信息。1D-cnn输入的channel和输出的channel大小皆为句子长度，意味着对每个位置的信息都进行筛选。将cnn的输出进行Maxpool之后连接一个全连接层，全连接层输出的大小为(batch_size,sequence_length,2),这样每个位置都得到一个代表起始位置和结束位置的分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn_roberta_models(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(Cnn_roberta_models, self).__init__()\n",
    "        self.bert = transformers.RobertaModel.from_pretrained('../input/roberta/roberta-base-pytorch_model.bin',config=config)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.start_end = nn.Linear(self.bert.config.hidden_size*2, 2)\n",
    "        self.cnn_input = nn.Conv1d(in_channels=140, out_channels=140, kernel_size=128, stride=1)\n",
    "        self.Linear = nn.Linear(in_features=320, out_features=2)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        torch.nn.init.normal_(self.Linear.weight, std=0.02)\n",
    "        self.cnn_256 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=256, stride=1)\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        hidden_state,_, out = self.bert(\n",
    "          input_ids=ids,\n",
    "          attention_mask=mask,\n",
    "          token_type_ids=token_type_ids\n",
    "        )      #tuple:(embedding(batch_size,sequence_length,hidden_size),output of each layer(batch_size,sequence_length,hidden_size))\n",
    "        hidden_state = self.drop(hidden_state)\n",
    "        out = self.cnn_input(hidden_state) #(batch_size, out_channels=140,  640)\n",
    "        out = self.maxpool(out) #（b,140,320 ）\n",
    "        out = self.Linear(out) #(b, 140, 2)\n",
    "        start, end = torch.split(out, split_size_or_sections=1, dim=-1)\n",
    "        start = torch.squeeze(start, dim=-1)\n",
    "        end = torch.squeeze(end, dim=-1)\n",
    "        return start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、模型训练与评估\n",
    "## 1.Loss \n",
    "    模型的输出为每个位置作为起始位置和结束位置的分数，将其softmax得到归一化的分数，训练的时候与已知的target一起，用CrossEntropy作为损失函数，总的Loss为起始位置与结束位置的Loss总和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    loss_fct = torch.nn.CrossEntropyLoss() #softmax+loss(分类器)\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.评估函数\n",
    "    使用Jaccard similarity作为评估标准：即两个句子所含单词的交集与并集的比值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    verbose=False):\n",
    "    \n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "\n",
    "    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "\n",
    "    jac = jaccard(target_string, filtered_output)\n",
    "    return jac, filtered_output\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(train_data, optimizer, EPOCHS):\n",
    "\n",
    "  total_steps = len(train_data) * EPOCHS\n",
    "  scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                          num_warmup_steps=0,\n",
    "                          num_training_steps=total_steps)   \n",
    "    #学习率设置为线性下降https://huggingface.co/transformers/main_classes/optimizer_schedules.html#learning-rate-schedules-pytorch\n",
    "  return scheduler\n",
    "def optimizer(model):\n",
    "  return AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "      model, \n",
    "      data_loader, \n",
    "      loss_fn, \n",
    "      optimizer, \n",
    "      device, \n",
    "      scheduler,\n",
    "      train_data\n",
    "     ):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    mean_jaccard = []\n",
    "    data_loader = tqdm(data_loader)\n",
    "    optimizer = optimizer(model)\n",
    "    scheduler = scheduler(train_data, optimizer, EPOCHS)\n",
    "\n",
    "    for i_batch,d in enumerate(data_loader):              #每个d就是一个batch\n",
    "        input_ids = d[\"ids\"].to(device)\n",
    "        attention_mask = d[\"mask\"].to(device)\n",
    "        token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "        target_start = d['targets_start'].to(device)\n",
    "        target_end = d['targets_end'].to(device)\n",
    "\n",
    "        tweet_text = d['tweet_text']\n",
    "        selected_text = d['selected_text']\n",
    "        offsets = d['offsets']\n",
    "        sentiment = d['sentiment']\n",
    "\n",
    "        start, end= model(\n",
    "          ids=input_ids,\n",
    "          mask=attention_mask,\n",
    "          token_type_ids=token_type_ids\n",
    "        )             #(batch_size, sequence_length)\n",
    "        loss = loss_fn(start_logits=start,\n",
    "                end_logits=end,\n",
    "                start_positions=target_start,\n",
    "                end_positions=target_end\n",
    "        )#crossentropy\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        #softmax得到每个位置概率的start和end，因为是batch_size的维度，所以此处不能直接max\n",
    "        start = torch.nn.functional.softmax(start, dim=1).cpu().detach().numpy()  #(batch)\n",
    "        end = torch.nn.functional.softmax(end, dim=1).cpu().detach().numpy()  #(batch)\n",
    "        jaccard_scores = []\n",
    "        #对batch里面的每条推特计算jaccard_score\n",
    "        for i, irow_tweet in enumerate(tweet_text):\n",
    "          i_selected_text = selected_text[i]\n",
    "          i_sentiment = sentiment[i]\n",
    "          i_start = np.argmax(start[i,:])\n",
    "          i_end = np.argmax(end[i,:])\n",
    "          i_offsets = offsets[i]\n",
    "          jaccard_score, _ = calculate_jaccard_score(\n",
    "                original_tweet=irow_tweet,\n",
    "                target_string=i_selected_text,\n",
    "                sentiment_val=i_sentiment,\n",
    "                idx_start=i_start,\n",
    "                idx_end=i_end,\n",
    "                offsets=i_offsets\n",
    "            )\n",
    "          jaccard_scores.append(jaccard_score)        #每条tweet的分数\n",
    "        mean_jaccard.append(np.mean(jaccard_scores))      #每个batch的平均分数\n",
    "       # print(f'第{i_batch+1}个batch: loss：{loss}, jaccard_score:{np.mean(mean_jaccard)}')\n",
    "\n",
    "      \n",
    "    return np.mean(mean_jaccard), np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    mean_jaccard = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader):\n",
    "          input_ids = d[\"ids\"].to(device)\n",
    "          attention_mask = d[\"mask\"].to(device)\n",
    "          token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "          target_start = d['targets_start'].to(device)\n",
    "          target_end = d['targets_end'].to(device)\n",
    "\n",
    "          tweet_text = d['tweet_text']\n",
    "          selected_text = d['selected_text']\n",
    "          offsets = d['offsets']\n",
    "          sentiment = d['sentiment']\n",
    "\n",
    "          start, end= model(\n",
    "            ids=input_ids,\n",
    "            mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "          )             #(batch_size, sequence_length)\n",
    "          loss = loss_fn(start_logits=start,\n",
    "                  end_logits=end,\n",
    "                  start_positions=target_start,\n",
    "                  end_positions=target_end\n",
    "          )#crossentropy\n",
    "          losses.append(loss.item())\n",
    "          #softmax得到每个位置概率的start和end，因为是batch_size的维度，所以此处不能直接max\n",
    "          start = torch.nn.functional.softmax(start, dim=1).cpu().detach().numpy()  #(batch)\n",
    "          end = torch.nn.functional.softmax(end, dim=1).cpu().detach().numpy()  #(batch)\n",
    "          jaccard_scores = []\n",
    "          #对batch里面的每条推特计算jaccard_score\n",
    "          for i, irow_tweet in enumerate(tweet_text):\n",
    "            i_selected_text = selected_text[i]\n",
    "            i_sentiment = sentiment[i]\n",
    "            i_start = np.argmax(start[i,:])\n",
    "            i_end = np.argmax(end[i,:])\n",
    "            i_offsets = offsets[i]\n",
    "            jaccard_score, _ = calculate_jaccard_score(\n",
    "                  original_tweet=irow_tweet,\n",
    "                  target_string=i_selected_text,\n",
    "                  sentiment_val=i_sentiment,\n",
    "                  idx_start=i_start,\n",
    "                  idx_end=i_end,\n",
    "                  offsets=i_offsets\n",
    "              )\n",
    "            jaccard_scores.append(jaccard_score)     #每条tweet的分数\n",
    "          mean_jaccard.append(np.mean(jaccard_scores))   #每个batch的平均分数\n",
    "\n",
    "    return np.mean(mean_jaccard), np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold_num,model):\n",
    "  #%%time\n",
    "  history = defaultdict(list)\n",
    "  best_accuracy = 0\n",
    "  train = [train_1, train_2, train_3, train_4, train_5]\n",
    "  train_data = train\n",
    "  print(len(train_data))\n",
    "  eval_data = train_data.pop(fold_num)\n",
    "  train_data = pd.concat(train_data).sample(frac=1)\n",
    "  train_data_loader = create_data_loader(train_data,\n",
    "                      tokenizer=tokenizer,\n",
    "                      max_len=MAX_LEN,\n",
    "                      batch_size=BATCH_SIZE\n",
    "                      )\n",
    "  eval_data_loader = create_data_loader(eval_data,\n",
    "                      tokenizer=tokenizer,\n",
    "                      max_len=MAX_LEN,\n",
    "                      batch_size=BATCH_SIZE\n",
    "                      )\n",
    "\n",
    "  print(f'Fold {fold_num}')\n",
    "  for epoch in range(EPOCHS):\n",
    "\n",
    "      print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "      print('-' * 10)\n",
    "\n",
    "      train_jaccard_score, train_loss = train_epoch(\n",
    "      model,\n",
    "      train_data_loader,    \n",
    "      loss_fn, \n",
    "      optimizer, \n",
    "      device, \n",
    "      scheduler,\n",
    "      train_data\n",
    "    )\n",
    "\n",
    "      print(f'Train loss:{train_loss} Train jaccard score:{train_jaccard_score}')\n",
    "\n",
    "      val_jaccard_score, val_loss = eval_model(\n",
    "      model,\n",
    "      eval_data_loader,\n",
    "      loss_fn, \n",
    "      device\n",
    "    )\n",
    "\n",
    "      print(f'Val   loss:{val_loss}  Val jaccard score:{val_jaccard_score}')\n",
    "      print()\n",
    "\n",
    "      history['train_jaccard_score'].append(train_jaccard_score)\n",
    "      history['train_loss'].append(train_loss)\n",
    "      history['val_jaccard_score'].append(val_jaccard_score)\n",
    "      history['val_loss'].append(val_loss)\n",
    "\n",
    "      if val_jaccard_score > best_accuracy:\n",
    "          torch.save(model.state_dict(), f'best_model_{fold_num}.bin')\n",
    "          best_accuracy = val_jaccard_score\n",
    "  torch.save(model.state_dict(),f'model_{fold_num}.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = []\n",
    "models2 = []\n",
    "config = transformers.BertConfig.from_pretrained('../input/roberta/roberta-base-config.json')\n",
    "config.output_hidden_state = True\n",
    "for fold in range(5):\n",
    "    model = Roberta_model_1(config)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(f'../input/5fold-roberta/model_{fold}.bin'))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "for fold in range(5):\n",
    "    model = Roberta_model_1(config)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(f'../input/lr0-5-fold0/5lr-3out-init-best_model_{fold}.bin'))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "for fold in range(5):\n",
    "    model = Cnn_roberta_models(config)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(f'../input/cnn01final/model_{fold}.bin'))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "for fold in range(10):\n",
    "    model = Roberta_model_2(config)\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(f'../input/tweet-sentiment-roberta-pytorch/roberta_fold{fold+1}.pth'))\n",
    "    model.eval()\n",
    "    models2.append(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、后处理与预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(models,models2,data_loader):\n",
    "\n",
    "  tweet_text = []\n",
    "  predict_selects = []\n",
    "  ori_tweet = []\n",
    "  coef =[[0.8, 1, 0.5, 2], [0.8, 1, 0.5, 2]]\n",
    "  #optimizer = optim.SGD(coef, lr=0.01, momentum=0.9)\n",
    "  \n",
    "  if True:\n",
    "        for d in tqdm(data_loader):\n",
    "          input_ids = d[\"ids\"].to(device)\n",
    "          attention_mask = d[\"mask\"].to(device)\n",
    "          token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "          #target_start = d['targets_start'].to(device)\n",
    "          #target_end = d['targets_end'].to(device)\n",
    "\n",
    "          tweet_text = d['tweet_text']\n",
    "          #selected_text = d['selected_text']\n",
    "          #offsets = d['offsets']\n",
    "          sentiment = d['sentiment']\n",
    "          batch_starts = []\n",
    "          batch_ends = []\n",
    "          with torch.no_grad():\n",
    "              for model in models:\n",
    "                    model.eval()\n",
    "                    start, end= model(\n",
    "                ids=input_ids,\n",
    "                mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "              )             #(batch_size, sequence_length)\n",
    "                    batch_starts.append(torch.unsqueeze(start, dim=0))\n",
    "                    batch_ends.append(torch.unsqueeze(end, dim=0))             #每个batch*每个模型的end[模型数，batch，句子长度]\n",
    "              \n",
    "              for model in models2:\n",
    "                    model.eval()\n",
    "                    start, end= model(\n",
    "                input_ids,\n",
    "                attention_mask\n",
    "              )             #(batch_size, sequence_length)\n",
    "                    batch_starts.append(torch.unsqueeze(start, dim=0))\n",
    "                    batch_ends.append(torch.unsqueeze(end, dim=0))             #每个batch*每个模型的end[模型数，batch，句子长度]\n",
    "              batch_starts = torch.cat(tuple(batch_starts))\n",
    "              batch_ends = torch.cat(tuple(batch_ends)) \n",
    "                \n",
    "                \n",
    "          \n",
    "          for i, irow_tweet in enumerate(tweet_text):  #对batch里的每个句子进行循环，算所有模型的后处理值\n",
    "              losses = []\n",
    "              starts = batch_starts[:,i,:]\n",
    "              ends = batch_ends[:,i,:]\n",
    "              starts_pp = []\n",
    "              ends_pp = []\n",
    "              #i_target_start = target_start[i,:]\n",
    "              #i_target_end = i_target_end[i, :]\n",
    "              for start in starts:\n",
    "           \n",
    "                start = torch.nn.functional.softmax(start, dim=-1).cpu().detach().numpy()   #得到每个位置是开始的概率\n",
    "                starts_pp.append(start)                         #[模型数， 句子长度]\n",
    "              for end in ends:\n",
    "                end = torch.nn.functional.softmax(end, dim=-1).cpu().detach().numpy()\n",
    "                ends_pp.append(end)\n",
    "              #optR = OptimizedRounder()\n",
    "              #optR.fit(starts_pp, target_start)\n",
    "              #coefficients = optR.coefficients()\n",
    "              i_start = predict(starts_pp, coef[0])\n",
    "              i_end = predict(ends_pp, coef[1])\n",
    "              #optimizer.zero_grad()\n",
    "              #loss = loss_fn(i_start, i_end, i_target_start, i_target_end)\n",
    "              #losses.append(loss)\n",
    "              #loss.backward()\n",
    "              #optimizer.step()\n",
    "         \n",
    "              \n",
    "   \n",
    "              i_sentiment = sentiment[i]\n",
    "    \n",
    "              predict_select = get_predict_text(\n",
    "                tweet=irow_tweet,\n",
    "                sentiment=i_sentiment,\n",
    "                start=i_start,\n",
    "                end=i_end\n",
    "            )\n",
    "              predict_selects.append(predict_select)\n",
    "              ori_tweet.append(irow_tweet)\n",
    "          #print(np.mean(losses))\n",
    "         # print(coef)\n",
    "              \n",
    "\n",
    "  \n",
    "  return ori_tweet, predict_selects\n",
    "def get_predict_text(tweet, sentiment, start, end):    #这里的start 和 end 是按token编号，且算上了问题和原文\n",
    "  sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "\n",
    "  tokenize_tweet = tokenizer.encode(tweet).ids   #原tweet\n",
    "  all_token = [0] + [sentiment_id[sentiment]] + [2] + [2] + tokenize_tweet + [2]  #问题和原文总的token\n",
    "  predict_text_token = all_token[start:end+1]\n",
    "  predict_text = tokenizer.decode(predict_text_token)\n",
    "  return predict_text\n",
    "def predict( start_pp, coef):\n",
    "        \n",
    "        start_final = np.zeros(MAX_LEN)\n",
    "        for j, start in enumerate(start_pp):     #对每个模型而言\n",
    "            for i in range(MAX_LEN):\n",
    "                if start[i] < np.median(start):\n",
    "                    start[i] = 0\n",
    "            if j <5:\n",
    "                start_final =start_final +  start*10000000*coef[0]\n",
    "            elif j>=5 and j<10:\n",
    "                start_final =start_final +  start*10000000*coef[1]\n",
    "            elif j>=10 and j<15 :\n",
    "                start_final =start_final +  start*10000000*coef[2]\n",
    "            elif j>15:\n",
    "                start_final =start_final +  start*10000000*coef[3]\n",
    "        start_final = np.array(start_final)/len(start_pp)\n",
    "        return np.argmax(start_final)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(s):\n",
    "    a = re.findall('[^A-Za-z0-9]',s)\n",
    "    b = re.sub('[^A-Za-z0-9]+', '', s)\n",
    "\n",
    "    try:\n",
    "        if a.count('.') >= 3:\n",
    "            text =b +\" \" + b + '. ' + b + '..'\n",
    "        elif a.count('!') >= 3:\n",
    "            text = b + \" \" + b + '! ' + b + '!! ' \n",
    "        elif a.count('?') >= 3:\n",
    "            text = b + \" \" + b + '? ' + b + '?? ' \n",
    "        else:\n",
    "            text = s\n",
    "        return text\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test.loc[:,'selected_text'] = pre_selects\n",
    "ds_test.selected_text.replace('', np.nan, inplace=True)\n",
    "ds_test.loc[ds_test.selected_text.isnull(),'selected_text'] = ds_test.loc[ds_test.selected_text.isnull(),'text']\n",
    "ds_test['selected_text'] = ds_test['selected_text'].apply(lambda x:post_process(x) if len(x.split())==1 else x)\n",
    "ds_test['selected_text'] = ds_test['selected_text'].apply(lambda x: post_process(x) if len(x.split())==1 else x)\n",
    "ds_test['selected_text'] = ds_test['selected_text'].apply(lambda x: post_process(x) if len(x.split())==1 else x)\n",
    "submit = pd.concat((ds_test.textID, ds_test.selected_text), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08a476cf394d4ad4b038d71360d96548": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9a67e344eaf944d4b50fbe6f0a35fd8f",
        "IPY_MODEL_a9ae554a54384979b18d5a2fff1c2fa8"
       ],
       "layout": "IPY_MODEL_eb69fdcd3c764623acebcd5864c95e1c"
      }
     },
     "3afb1dbb333244bcb2d04cc4df7ae1c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4e0ce24cfd2f49c49d8c84c4423c3132": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "533d6c1a0c8f4db0b43539847aafee8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "9a67e344eaf944d4b50fbe6f0a35fd8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4e0ce24cfd2f49c49d8c84c4423c3132",
       "max": 221,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_533d6c1a0c8f4db0b43539847aafee8d",
       "value": 221
      }
     },
     "a9ae554a54384979b18d5a2fff1c2fa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3afb1dbb333244bcb2d04cc4df7ae1c3",
       "placeholder": "​",
       "style": "IPY_MODEL_aa61044802414ebb9b547166f134dbee",
       "value": " 221/221 [28:51&lt;00:00,  7.83s/it]"
      }
     },
     "aa61044802414ebb9b547166f134dbee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "eb69fdcd3c764623acebcd5864c95e1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
